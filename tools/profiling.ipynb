{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This jupyter file is used for LiDAR detection efficiency profiling\n",
    "\n",
    "## 1. Import required module from OpenPCDet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5\"\n",
    "import _init_path\n",
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from test import repeat_eval_ckpt\n",
    "\n",
    "import spconv.pytorch as spconv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from pcdet.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "from pcdet.datasets import build_dataloader\n",
    "from pcdet.models import build_network, model_fn_decorator\n",
    "from pcdet.utils import common_utils\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the waymo data for profiling\n",
    "\n",
    "**You can also re-use the dataloader defined in OpenPCDet (Recommanded)**\n",
    "\n",
    "If you choose this option, please skip this part.\n",
    "\n",
    "**Another option**\n",
    "\n",
    "You can construct your own code for loading point cloud from waymo dataset. Check the code in ```../pcdet/datasets/waymo/waymo_dataset.py``` as reference. For the initial stage, loading one LiDAR scan would be enough. While later, we may need to load more to average the measurement for better accuracy.\n",
    "\n",
    "The waymo data is located in ```../data/waymo```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct for you data loading function here\n",
    "\n",
    "dataset_base_path = '/home/jnd/code/OpenPCDet/data/waymo'\n",
    "\n",
    "def load_waymo_pcd():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model based on cfg files\n",
    "\n",
    "In this section, build the model following the code in ```./train.py```. The model config file in located in ```./cfgs/waymo_models```. The required configs are: pointpillar, pvrcnn, second, centerpoint. \n",
    "\n",
    "For profiling the efficiency of operations, you can just take the specific layer of the model for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_build_data(model_name, batch_size):\n",
    "    cfg_file = './cfgs/waymo_models/{}.yaml'.format(model_name)\n",
    "\n",
    "    output_dir = Path('./profiling_log')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    log_file = output_dir / ('log_train_%s.txt' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "    logger = common_utils.create_logger(log_file, rank=0)\n",
    "    cfg_from_yaml_file(cfg_file, cfg)\n",
    "\n",
    "    # It would take some time to initialize the dataset.\n",
    "\n",
    "    test_set, test_loader, test_sampler = build_dataloader(\n",
    "            dataset_cfg=cfg.DATA_CONFIG,\n",
    "            class_names=cfg.CLASS_NAMES,\n",
    "            batch_size=batch_size,\n",
    "            dist=False, workers=1,\n",
    "            logger=logger,\n",
    "            training=False,  # I would recommand you to use a test loader\n",
    "            merge_all_iters_to_one_epoch=True,\n",
    "            total_epochs=1,\n",
    "            seed=666\n",
    "        )\n",
    "\n",
    "    model = build_network(model_cfg=cfg.MODEL, num_class=3, dataset=test_set)\n",
    "    return model, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Now load the data and feed to the network for profiling\n",
    "\n",
    "There're two options for you to perform this task:\n",
    "\n",
    "1. Use your own data loading function and feed the data to the network\n",
    "2. Re-use the dataset to load the data. Check function **eval_one_epoch** in file ```./eval_utils/eval_utils.py``` for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def rearrange_df(df1, df2, batch_size):\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    active1 = df1['active_bytes'].copy()\n",
    "    active2 = df2['active_bytes'].copy()\n",
    "    reserved1 = df1['reserved_bytes'].copy()\n",
    "    reserved2 = df2['reserved_bytes'].copy()\n",
    "    for i in range(len(active1)):\n",
    "        if not isinstance(active1[i], float) and not isinstance(reserved1[i], float):        \n",
    "            if active1[i][-1] == 'G':\n",
    "                active1[i] = float(active1[i][:-1]) * 1024\n",
    "            \n",
    "            elif active1[i][-1] == 'K':\n",
    "                active1[i] = float(active1[i][:-1]) / 1024\n",
    "            \n",
    "            else:\n",
    "                active1[i] = float(active1[i][:-1])\n",
    "\n",
    "            if reserved1[i][-1] == 'G':\n",
    "                reserved1[i] = float(reserved1[i][:-1]) * 1024    \n",
    "                \n",
    "            elif reserved1[i][-1] == 'K':\n",
    "                reserved1[i] = float(reserved1[i][:-1]) / 1024\n",
    "  \n",
    "            else:\n",
    "                reserved1[i] = float(reserved1[i][:-1])\n",
    "\n",
    "    for i in range(len(active2)):\n",
    "        if not isinstance(active2[i], float) and not isinstance(reserved2[i], float):        \n",
    "            if active2[i][-1] == 'G':\n",
    "                active2[i] = float(active2[i][:-1]) * 1024\n",
    "            \n",
    "            elif active2[i][-1] == 'K':\n",
    "                active2[i] = float(active2[i][:-1]) / 1024\n",
    "            \n",
    "            else:\n",
    "                active2[i] = float(active2[i][:-1])\n",
    "\n",
    "            if reserved2[i][-1] == 'G':\n",
    "                reserved2[i] = float(reserved2[i][:-1]) * 1024    \n",
    "                \n",
    "            elif reserved2[i][-1] == 'K':\n",
    "                reserved2[i] = float(reserved2[i][:-1]) / 1024\n",
    "  \n",
    "            else:\n",
    "                reserved2[i] = float(reserved2[i][:-1])\n",
    "                \n",
    "    new_df['active_bytes'] = (active1 - active2) / (batch_size - 1)\n",
    "    new_df['reserved_bytes'] = reserved2\n",
    "    new_df = new_df.astype('float').round(3).fillna(\"\")\n",
    "    new_df = new_df.astype('str')\n",
    "    new_df[(new_df['active_bytes']!=\"\") & (new_df['reserved_bytes']!=\"\")] = new_df[(new_df['active_bytes']!=\"\") & (new_df['reserved_bytes']!=\"\")] + 'M'\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pytorch_memlab import LineProfiler\n",
    "import logging\n",
    "from pcdet.models import load_data_to_gpu\n",
    "\n",
    "final_dict = {'overall_df': None}\n",
    "\n",
    "def gpu_warm_up(model_name):\n",
    "    model, test_loader = load_and_build_data(model_name, 1)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    warm_up_data = next(iter(test_loader))\n",
    "    load_data_to_gpu(warm_up_data)\n",
    "    for _ in range(20):\n",
    "        try:\n",
    "            res1, res2 = model(warm_up_data)\n",
    "        except:\n",
    "            print(\"PointRCNN is so annoying.\")\n",
    "    del model, test_loader, res1, res2\n",
    "    del warm_up_data\n",
    "\n",
    "def inference_simulation(model_name, batch_size):\n",
    "    \n",
    "    model, test_loader = load_and_build_data(model_name, batch_size)\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    one_data = None\n",
    "\n",
    "    for i, batch_dict in enumerate(test_loader):\n",
    "        load_data_to_gpu(batch_dict)\n",
    "        one_data = batch_dict.copy()\n",
    "        print(batch_dict.keys())\n",
    "        gpu_warm_up(model_name)\n",
    "        with torch.no_grad():\n",
    "            with LineProfiler(model.forward) as overall:\n",
    "                starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "                starter.record()\n",
    "                pred_dicts, ret_dict = model(batch_dict)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "            \n",
    "            model_df = pd.read_html(overall.display()._repr_html_())[0]\n",
    "            model_df.columns = [i[0] for i in model_df.columns]\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            logging.info(\"|---------------------------------------------- The model is {} --------------------------------------------|\\n\".format(model.__class__.__name__))\n",
    "            logging.info(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> The runtime of {} is {} ms.\".format(model.__class__.__name__, curr_time))\n",
    "            final_dict[\"overall_df\"] = model_df\n",
    "            logging.info(\">>>> Analyzing the memory usage of this model with the batch size 1 <<<<\")\n",
    "            logging.info(overall.display())\n",
    "            \n",
    "        else:\n",
    "            model_df[['active_bytes', 'reserved_bytes']] = rearrange_df(model_df, final_dict['overall_df'], batch_size)\n",
    "            logging.info(\">>>> Now we analyze the average memory usage of this model <<<<\")\n",
    "            logging.info('\\n' + str(model_df))\n",
    "        break\n",
    "    return model, one_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Record execution time and memory usage\n",
    "\n",
    "For execution time, you could simply use the ```time``` library to record it. \n",
    "\n",
    "As for memory usage, it's a bit more tricky. But we have two interesting libraries for you to explore:\n",
    "\n",
    "1. [pytorch_memlab](https://github.com/stonesjtu/pytorch_memlab)\n",
    "2. [pytorch_profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)\n",
    "\n",
    "**Please read the document and understand how they work before use it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiling_one_batch(model_name):\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "    model, one_data = inference_simulation(model_name, batch_size=1)\n",
    "\n",
    "    vfe = model.vfe\n",
    "    backbone_3d = model.backbone_3d\n",
    "    map2bev = model.map_to_bev_module\n",
    "    backbone_2d = model.backbone_2d\n",
    "    dense_head = model.dense_head\n",
    "    pfe = model.pfe\n",
    "    point_head = model.point_head\n",
    "    roi_head = model.roi_head\n",
    "    \n",
    "    profile_dict = {'vfe_time': 0, \n",
    "                    'pfn_time': 0, \n",
    "                    'vfe_df': None,\n",
    "                    'backbone_3d_time': 0, \n",
    "                    'spconv1_t': 0, \n",
    "                    'spconv2_t': 0, \n",
    "                    'spconv3_t': 0, \n",
    "                    'spconv4_t': 0, \n",
    "                    'backbone_3d_df': None,\n",
    "                    'MSG_SA_time': 0,\n",
    "                    'MSG_FP_time': 0,\n",
    "                    'map2bev_time': 0, \n",
    "                    'map2bev_df': None, \n",
    "                    'pfe_time': 0, \n",
    "                    'pfe_df': None, \n",
    "                    'pfe_grouping_t': 0, \n",
    "                    'pfe_grouping_df': None,\n",
    "                    'pfe_bev_feature_t': 0, \n",
    "                    'pfe_bev_df': None,\n",
    "                    'pfe_raw_feature_t': 0, \n",
    "                    'pfe_raw_df': None,\n",
    "                    'pfe_mlp_t':[],\n",
    "                    'pfe_mlp_df':[],\n",
    "                    'backbone_2d_time':0, \n",
    "                    'backbone_2d_df': None,\n",
    "                    'dense_head_time':0, \n",
    "                    'dense_head_df': None,\n",
    "                    'center_head_time': 0,\n",
    "                    'point_head_time': 0,\n",
    "                    'point_head_df': None,\n",
    "                    'roi_head_time': 0,\n",
    "                    'roi_head_df': None,\n",
    "                    'roi_pooling_time': 0,\n",
    "                    'roi_pooling_df': None,\n",
    "                    'point_rcnn_proposal_time': 0,\n",
    "                    'point_rcnn_proposal_df': None\n",
    "                    }\n",
    "    \n",
    "    if vfe is not None:\n",
    "        if model.vfe.__class__.__name__ == \"MeanVFE\":\n",
    "            with LineProfiler(vfe.forward) as prof:\n",
    "                start_time = time.time()\n",
    "                out = vfe.forward(one_data)\n",
    "                profile_dict['vfe_time'] = (time.time() - start_time)\n",
    "\n",
    "\n",
    "        elif model.vfe.__class__.__name__ == \"PillarVFE\":\n",
    "            with LineProfiler(vfe.forward) as prof:\n",
    "                start_time = time.time()\n",
    "                out = vfe.forward(one_data)\n",
    "                profile_dict['vfe_time'] = (time.time() - start_time)\n",
    "\n",
    "\n",
    "            # Analyze pfn process\n",
    "            voxel_features, voxel_num_points, coords = one_data['voxels'], one_data['voxel_num_points'], one_data['voxel_coords']\n",
    "            points_mean = voxel_features[:, :, :3].sum(dim=1, keepdim=True) / voxel_num_points.type_as(voxel_features).view(-1, 1, 1)\n",
    "            f_cluster = voxel_features[:, :, :3] - points_mean\n",
    "            f_center = torch.zeros_like(voxel_features[:, :, :3])\n",
    "            f_center[:, :, 0] = voxel_features[:, :, 0] - (coords[:, 3].to(voxel_features.dtype).unsqueeze(1) * vfe.voxel_x + vfe.x_offset)\n",
    "            f_center[:, :, 1] = voxel_features[:, :, 1] - (coords[:, 2].to(voxel_features.dtype).unsqueeze(1) * vfe.voxel_y + vfe.y_offset)\n",
    "            f_center[:, :, 2] = voxel_features[:, :, 2] - (coords[:, 1].to(voxel_features.dtype).unsqueeze(1) * vfe.voxel_z + vfe.z_offset)\n",
    "            if vfe.use_absolute_xyz:\n",
    "                features = [voxel_features, f_cluster, f_center]\n",
    "            else:\n",
    "                features = [voxel_features[..., 3:], f_cluster, f_center]\n",
    "            if vfe.with_distance:\n",
    "                points_dist = torch.norm(voxel_features[:, :, :3], 2, 2, keepdim=True)\n",
    "                features.append(points_dist)\n",
    "            features = torch.cat(features, dim=-1)\n",
    "            voxel_count = features.shape[1]\n",
    "            mask = vfe.get_paddings_indicator(voxel_num_points, voxel_count, axis=0)\n",
    "            mask = torch.unsqueeze(mask, -1).type_as(voxel_features)\n",
    "            features *= mask\n",
    "            start_time = time.time()\n",
    "            for pfn in vfe.pfn_layers:\n",
    "                features = pfn(features)\n",
    "            profile_dict['pfn_time'] = time.time() - start_time\n",
    "                       \n",
    "            del voxel_features, voxel_num_points, coords\n",
    "            del points_mean\n",
    "            del f_cluster\n",
    "            del f_center\n",
    "            del features\n",
    "            del voxel_count\n",
    "            del mask\n",
    "            \n",
    "        vfe_df = pd.read_html(prof.display()._repr_html_())[0]\n",
    "        vfe_df.columns = [i[0] for i in vfe_df.columns]\n",
    "        profile_dict['vfe_df'] = vfe_df\n",
    "\n",
    "    else: \n",
    "        out = one_data\n",
    "        \n",
    "    if backbone_3d is not None:\n",
    "        with LineProfiler(backbone_3d.forward) as prof2:\n",
    "            start_time = time.time()\n",
    "            out2 = backbone_3d.forward(out)\n",
    "            profile_dict['backbone_3d_time'] = (time.time() - start_time)\n",
    "            \n",
    "            backbone_3d_df = pd.read_html(prof2.display()._repr_html_())[0]\n",
    "            backbone_3d_df.columns = [i[0] for i in backbone_3d_df.columns]\n",
    "            profile_dict['backbone_3d_df'] = backbone_3d_df\n",
    "        \n",
    "        if \"Voxel\" in backbone_3d.__class__.__name__:       \n",
    "            # Analyze specific process\n",
    "            voxel_features, voxel_coords = out['voxel_features'], out['voxel_coords']\n",
    "            batch_size = out['batch_size']\n",
    "            input_sp_tensor = spconv.SparseConvTensor(\n",
    "                    features=voxel_features,\n",
    "                    indices=voxel_coords.int(),\n",
    "                    spatial_shape=model.backbone_3d.sparse_shape,\n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "\n",
    "            x = model.backbone_3d.conv_input(input_sp_tensor)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            conv1 = backbone_3d.conv1(x)\n",
    "            profile_dict['spconv1_t'] = (time.time() - start_time)\n",
    "\n",
    "            start_time2 = time.time()\n",
    "            conv2 = backbone_3d.conv2(conv1)\n",
    "            profile_dict['spconv2_t'] = (time.time() - start_time2)\n",
    "\n",
    "            start_time3 = time.time()\n",
    "            conv3 = backbone_3d.conv3(conv2)\n",
    "            profile_dict['spconv3_t'] = (time.time() - start_time3)\n",
    "\n",
    "            start_time4 = time.time()\n",
    "            conv4 = backbone_3d.conv4(conv3)\n",
    "            end_time = time.time()\n",
    "            profile_dict['spconv4_t'] = (end_time - start_time4)\n",
    "\n",
    "            del voxel_features, voxel_coords\n",
    "            del input_sp_tensor\n",
    "            del x\n",
    "            del conv1\n",
    "            del conv2\n",
    "            del conv3\n",
    "            del conv4\n",
    "\n",
    "        elif \"Point\" in backbone_3d.__class__.__name__:\n",
    "            \n",
    "            batch_size = out['batch_size']\n",
    "            points = out['points']\n",
    "            batch_idx, xyz, features = backbone_3d.break_up_pc(points)\n",
    "\n",
    "            xyz_batch_cnt = xyz.new_zeros(batch_size).int()\n",
    "            for bs_idx in range(batch_size):\n",
    "                xyz_batch_cnt[bs_idx] = (batch_idx == bs_idx).sum()\n",
    "\n",
    "            assert xyz_batch_cnt.min() == xyz_batch_cnt.max()\n",
    "            xyz = xyz.view(batch_size, -1, 3)\n",
    "            features = features.view(batch_size, -1, features.shape[-1]).permute(0, 2, 1).contiguous() if features is not None else None\n",
    "\n",
    "            l_xyz, l_features = [xyz], [features]\n",
    "            start_time = time.time()\n",
    "            for i in range(len(backbone_3d.SA_modules)):\n",
    "                li_xyz, li_features = backbone_3d.SA_modules[i](l_xyz[i], l_features[i])\n",
    "                l_xyz.append(li_xyz)\n",
    "                l_features.append(li_features)\n",
    "            profile_dict['MSG_SA_time'] = time.time() - start_time\n",
    "\n",
    "            start_time = time.time()\n",
    "            for i in range(-1, -(len(backbone_3d.FP_modules) + 1), -1):\n",
    "                l_features[i - 1] = backbone_3d.FP_modules[i](\n",
    "                    l_xyz[i - 1], l_xyz[i], l_features[i - 1], l_features[i]\n",
    "                )  # (B, C, N)\n",
    "            profile_dict['MSG_FP_time'] = time.time() - start_time\n",
    "            \n",
    "            del points\n",
    "            del batch_idx, xyz, features\n",
    "            del xyz_batch_cnt\n",
    "            del l_xyz, l_features\n",
    "            \n",
    "    else:\n",
    "        out2 = out\n",
    "\n",
    "    if map2bev is not None:\n",
    "        with LineProfiler(map2bev.forward) as prof3:\n",
    "            start_time = time.time()\n",
    "            out3 = map2bev.forward(out2)\n",
    "            profile_dict['map2bev_time'] = (time.time() - start_time)\n",
    "\n",
    "        map2bev_df = pd.read_html(prof3.display()._repr_html_())[0]\n",
    "        map2bev_df.columns = [i[0] for i in map2bev_df.columns]\n",
    "        profile_dict['map2bev_df'] = map2bev_df   \n",
    "    else:\n",
    "        out3 = out2\n",
    "\n",
    "\n",
    "    if pfe is not None:\n",
    "        with LineProfiler(pfe.forward) as prof4:\n",
    "            start_time = time.time()\n",
    "            out4 = pfe.forward(out3)\n",
    "            profile_dict['pfe_time'] = (time.time() - start_time)\n",
    "\n",
    "        pfe_df = pd.read_html(prof4.display()._repr_html_())[0]\n",
    "        pfe_df.columns = [i[0] for i in pfe_df.columns]\n",
    "        profile_dict['pfe_df'] = pfe_df\n",
    "\n",
    "        with LineProfiler(pfe.get_sampled_points) as grouping:\n",
    "            start_time_grouping = time.time()\n",
    "            keypoints = pfe.get_sampled_points(out3)\n",
    "            profile_dict['pfe_grouping_t'] = time.time() - start_time_grouping\n",
    "\n",
    "        pfe_grouping_df = pd.read_html(grouping.display()._repr_html_())[0]\n",
    "        pfe_grouping_df.columns = [i[0] for i in pfe_grouping_df.columns]\n",
    "        profile_dict['pfe_grouping_df'] = pfe_grouping_df\n",
    "\n",
    "        point_features_list = []\n",
    "        if 'bev' in pfe.model_cfg.FEATURES_SOURCE:\n",
    "            with LineProfiler(pfe.interpolate_from_bev_features) as bev:\n",
    "                start_time_bev = time.time()\n",
    "                point_bev_features = pfe.interpolate_from_bev_features(\n",
    "                    keypoints, out3['spatial_features'], out3['batch_size'],\n",
    "                    bev_stride=out3['spatial_features_stride']\n",
    "                )\n",
    "                profile_dict['pfe_bev_feature_t'] = time.time() - start_time_bev\n",
    "\n",
    "            pfe_bev_df = pd.read_html(bev.display()._repr_html_())[0]\n",
    "            pfe_bev_df.columns = [i[0] for i in pfe_bev_df.columns]\n",
    "            profile_dict['pfe_bev_df'] = pfe_bev_df\n",
    "            point_features_list.append(point_bev_features)\n",
    "            \n",
    "            del point_bev_features\n",
    "\n",
    "        batch_size = out3['batch_size']\n",
    "        new_xyz = keypoints[:, 1:4].contiguous()\n",
    "        new_xyz_batch_cnt = new_xyz.new_zeros(batch_size).int()\n",
    "        for k in range(batch_size):\n",
    "            new_xyz_batch_cnt[k] = (keypoints[:, 0] == k).sum()\n",
    "\n",
    "        if 'raw_points' in pfe.model_cfg.FEATURES_SOURCE:\n",
    "            raw_points = out3['points']\n",
    "            with LineProfiler(pfe.aggregate_keypoint_features_from_one_source) as raw:\n",
    "                start_time_raw = time.time()\n",
    "                pooled_features = pfe.aggregate_keypoint_features_from_one_source(\n",
    "                    batch_size=batch_size, aggregate_func=pfe.SA_rawpoints,\n",
    "                    xyz=raw_points[:, 1:4],\n",
    "                    xyz_features=raw_points[:, 4:].contiguous() if raw_points.shape[1] > 4 else None,\n",
    "                    xyz_bs_idxs=raw_points[:, 0],\n",
    "                    new_xyz=new_xyz, new_xyz_batch_cnt=new_xyz_batch_cnt,\n",
    "                    filter_neighbors_with_roi=pfe.model_cfg.SA_LAYER['raw_points'].get('FILTER_NEIGHBOR_WITH_ROI', False),\n",
    "                    radius_of_neighbor=pfe.model_cfg.SA_LAYER['raw_points'].get('RADIUS_OF_NEIGHBOR_WITH_ROI', None),\n",
    "                    rois=out3.get('rois', None)\n",
    "                )\n",
    "                profile_dict['pfe_raw_feature_t'] = time.time() - start_time_raw\n",
    "\n",
    "            pfe_raw_df = pd.read_html(bev.display()._repr_html_())[0]\n",
    "            pfe_raw_df.columns = [i[0] for i in pfe_raw_df.columns]\n",
    "            profile_dict['pfe_raw_df'] = pfe_raw_df\n",
    "            point_features_list.append(pooled_features)\n",
    "            \n",
    "            del pooled_features\n",
    "\n",
    "        for k, src_name in enumerate(pfe.SA_layer_names):\n",
    "            cur_coords = out3['multi_scale_3d_features'][src_name].indices\n",
    "            cur_features = out3['multi_scale_3d_features'][src_name].features.contiguous()\n",
    "            xyz = common_utils.get_voxel_centers(\n",
    "                cur_coords[:, 1:4], downsample_times=pfe.downsample_times_map[src_name],\n",
    "                voxel_size=pfe.voxel_size, point_cloud_range=pfe.point_cloud_range\n",
    "            )\n",
    "\n",
    "            with LineProfiler(pfe.aggregate_keypoint_features_from_one_source) as mlp:\n",
    "                start_time_mlp = time.time()\n",
    "                pooled_features = pfe.aggregate_keypoint_features_from_one_source(\n",
    "                    batch_size=batch_size, aggregate_func=pfe.SA_layers[k],\n",
    "                    xyz=xyz.contiguous(), xyz_features=cur_features, xyz_bs_idxs=cur_coords[:, 0],\n",
    "                    new_xyz=new_xyz, new_xyz_batch_cnt=new_xyz_batch_cnt,\n",
    "                    filter_neighbors_with_roi=pfe.model_cfg.SA_LAYER[src_name].get('FILTER_NEIGHBOR_WITH_ROI', False),\n",
    "                    radius_of_neighbor=pfe.model_cfg.SA_LAYER[src_name].get('RADIUS_OF_NEIGHBOR_WITH_ROI', None),\n",
    "                    rois=out3.get('rois', None)\n",
    "                )\n",
    "                profile_dict['pfe_mlp_t'].append(time.time() - start_time_mlp)\n",
    "\n",
    "            pfe_mlp_df = pd.read_html(mlp.display()._repr_html_())[0]\n",
    "            pfe_mlp_df.columns = [i[0] for i in pfe_mlp_df.columns]\n",
    "            profile_dict['pfe_mlp_df'].append(pfe_mlp_df)\n",
    "\n",
    "            point_features_list.append(pooled_features)\n",
    "        \n",
    "        del point_features_list\n",
    "        del keypoints\n",
    "        del new_xyz\n",
    "        del new_xyz_batch_cnt\n",
    "        del cur_coords\n",
    "        del cur_features\n",
    "        del xyz\n",
    "        del pooled_features\n",
    "    else:\n",
    "        out4 = out3\n",
    "\n",
    "\n",
    "    if backbone_2d is not None:\n",
    "        with LineProfiler(backbone_2d.forward) as prof5:\n",
    "            start_time = time.time()\n",
    "            out5 = backbone_2d.forward(out4)\n",
    "            profile_dict['backbone_2d_time'] = (time.time() - start_time)\n",
    "\n",
    "        backbone_2d_df = pd.read_html(prof5.display()._repr_html_())[0]\n",
    "        backbone_2d_df.columns = [i[0] for i in backbone_2d_df.columns]\n",
    "        profile_dict['backbone_2d_df'] = backbone_2d_df\n",
    "    else:\n",
    "        out5=out4\n",
    "\n",
    "\n",
    "    if dense_head is not None:\n",
    "        with LineProfiler(dense_head.forward) as prof6:\n",
    "            start_time = time.time()\n",
    "            out6 = dense_head.forward(out5)\n",
    "            profile_dict['dense_head_time'] = (time.time() - start_time)\n",
    "\n",
    "        if dense_head.__class__.__name__ == \"CenterHead\":\n",
    "            spatial_features_2d = out5['spatial_features_2d']\n",
    "            x = dense_head.shared_conv(spatial_features_2d)\n",
    "            pred_dicts = []\n",
    "            start_time = time.time()\n",
    "            for head in dense_head.heads_list:\n",
    "                pred_dicts.append(head(x))\n",
    "            profile_dict['center_head_time'] = time.time() - start_time\n",
    "            \n",
    "            del spatial_features_2d\n",
    "            del x \n",
    "            del pred_dicts\n",
    "            \n",
    "        dense_head_df = pd.read_html(prof6.display()._repr_html_())[0] \n",
    "        dense_head_df.columns = [i[0] for i in dense_head_df.columns]\n",
    "        profile_dict['dense_head_df'] = dense_head_df\n",
    "    else:\n",
    "        out6 = out5\n",
    "\n",
    "\n",
    "    if point_head is not None:\n",
    "        with LineProfiler(point_head.forward) as ph:\n",
    "            start_time = time.time()\n",
    "            out7 = point_head.forward(out6)\n",
    "            profile_dict['point_head_time'] = (time.time() - start_time)\n",
    "        point_head_df = pd.read_html(ph.display()._repr_html_())[0]\n",
    "        point_head_df.columns = [i[0] for i in point_head_df.columns]\n",
    "        profile_dict['point_head_df'] = point_head_df\n",
    "    else:\n",
    "        out7 = out6\n",
    "    \n",
    "    if roi_head is not None:\n",
    "\n",
    "        with LineProfiler(roi_head.forward) as roi:\n",
    "            start_time = time.time()\n",
    "            out8 = roi_head.forward(out7)\n",
    "            profile_dict['roi_head_time'] = (time.time() - start_time)\n",
    "        roi_head_df = pd.read_html(roi.display()._repr_html_())[0]\n",
    "        roi_head_df.columns = [i[0] for i in roi_head_df.columns]\n",
    "        profile_dict['roi_head_df'] = roi_head_df\n",
    "\n",
    "        if 'PV' in roi_head.__class__.__name__:\n",
    "            with LineProfiler(roi_head.roi_grid_pool) as pooling:\n",
    "                start_time_pooling = time.time()\n",
    "                pooled_features = roi_head.roi_grid_pool(out7)\n",
    "                profile_dict['roi_pooling_time'] = time.time() - start_time_pooling\n",
    "\n",
    "            roi_pooling_df = pd.read_html(pooling.display()._repr_html_())[0]\n",
    "            roi_pooling_df.columns = [i[0] for i in roi_pooling_df.columns]\n",
    "            profile_dict['roi_pooling_df'] = roi_pooling_df\n",
    "        \n",
    "        elif 'PointRCNN' in roi_head.__class__.__name__:\n",
    "            play_data = out7.copy()\n",
    "            with LineProfiler(roi_head.proposal_layer) as proposal:\n",
    "                start_time = time.time()\n",
    "                targets_dict = roi_head.proposal_layer(play_data, nms_config=roi_head.model_cfg.NMS_CONFIG['TRAIN' if roi_head.training else 'TEST'])\n",
    "                profile_dict['point_rcnn_proposal_time'] = time.time() - start_time\n",
    "            \n",
    "            point_rcnn_proposal_df = pd.read_html(proposal.display()._repr_html_())[0]\n",
    "            point_rcnn_proposal_df.columns = [i[0] for i in point_rcnn_proposal_df.columns]\n",
    "            profile_dict['point_rcnn_proposal_df'] = point_rcnn_proposal_df\n",
    "            \n",
    "            del play_data\n",
    "            \n",
    "    else:\n",
    "        out8 = out7\n",
    "        \n",
    "    return profile_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiling_multi_batch(model_name, batch_size, profile_dict):\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "    model, one_data = inference_simulation(model_name, batch_size)\n",
    "    vfe = model.vfe\n",
    "    backbone_3d = model.backbone_3d\n",
    "    map2bev = model.map_to_bev_module\n",
    "    backbone_2d = model.backbone_2d\n",
    "    dense_head = model.dense_head\n",
    "    pfe = model.pfe\n",
    "    point_head = model.point_head\n",
    "    roi_head = model.roi_head\n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"|||||||||||||||||||||||||||||||||||| VFE Part ||||||||||||||||||||||||||||||||||||\")\n",
    "    logging.info(\"\\n\")\n",
    "    if vfe is not None:\n",
    "        logging.info(\"----------------------- Analyzing on {} -----------------------\".format(model.vfe.__class__.__name__))\n",
    "        if model.vfe.__class__.__name__ == \"MeanVFE\":\n",
    "            with LineProfiler(vfe.forward) as prof:\n",
    "                start_time = time.time()\n",
    "                out = vfe.forward(one_data)\n",
    "                total_time = (time.time() - start_time)\n",
    "                logging.info(\">>>>>>>>>>>>>> The runtime for {} is {} ms\".format(model.vfe.__class__.__name__, profile_dict['vfe_time'] * 1000))\n",
    "        elif model.vfe.__class__.__name__ == \"PillarVFE\":\n",
    "            with LineProfiler(vfe.forward) as prof:\n",
    "                start_time = time.time()\n",
    "                out = vfe.forward(one_data)\n",
    "                total_time = (time.time() - start_time)\n",
    "                logging.info(\">>>>>>>>>>>>>> The runtime for {} is {} ms\".format(model.vfe.__class__.__name__, profile_dict['vfe_time'] * 1000))\n",
    "            # Analyze pfn process\n",
    "            voxel_features, voxel_num_points, coords = one_data['voxels'], one_data['voxel_num_points'], one_data['voxel_coords']\n",
    "            points_mean = voxel_features[:, :, :3].sum(dim=1, keepdim=True) / voxel_num_points.type_as(voxel_features).view(-1, 1, 1)\n",
    "            f_cluster = voxel_features[:, :, :3] - points_mean\n",
    "            f_center = torch.zeros_like(voxel_features[:, :, :3])\n",
    "            f_center[:, :, 0] = voxel_features[:, :, 0] - (coords[:, 3].to(voxel_features.dtype).unsqueeze(1) * vfe.voxel_x + vfe.x_offset)\n",
    "            f_center[:, :, 1] = voxel_features[:, :, 1] - (coords[:, 2].to(voxel_features.dtype).unsqueeze(1) * vfe.voxel_y + vfe.y_offset)\n",
    "            f_center[:, :, 2] = voxel_features[:, :, 2] - (coords[:, 1].to(voxel_features.dtype).unsqueeze(1) * vfe.voxel_z + vfe.z_offset)\n",
    "            if vfe.use_absolute_xyz:\n",
    "                features = [voxel_features, f_cluster, f_center]\n",
    "            else:\n",
    "                features = [voxel_features[..., 3:], f_cluster, f_center]\n",
    "            if vfe.with_distance:\n",
    "                points_dist = torch.norm(voxel_features[:, :, :3], 2, 2, keepdim=True)\n",
    "                features.append(points_dist)\n",
    "            features = torch.cat(features, dim=-1)\n",
    "            voxel_count = features.shape[1]\n",
    "            mask = vfe.get_paddings_indicator(voxel_num_points, voxel_count, axis=0)\n",
    "            mask = torch.unsqueeze(mask, -1).type_as(voxel_features)\n",
    "            features *= mask\n",
    "            start_time = time.time()\n",
    "            for pfn in vfe.pfn_layers:\n",
    "                features = pfn(features)\n",
    "            total_time = time.time() - start_time\n",
    "            logging.info(\">>>>>>>>>>>>>> There are {} PFNLayers\".format(len(vfe.pfn_layers)))\n",
    "            logging.info(\">>>>>>>>>>>>>> The total runtime of all PFNLayers is {} ms\".format(total_time * 1000))\n",
    "            logging.info(\">>>>>>>>>>>>>> The average runtime of each PFNLayer is {} ms\".format(total_time * 1000 / len(vfe.pfn_layers)))\n",
    "            logging.info(\"\\n\")\n",
    "            \n",
    "            del voxel_features, voxel_num_points, coords\n",
    "            del points_mean\n",
    "            del f_cluster\n",
    "            del f_center\n",
    "            del features\n",
    "            del voxel_count\n",
    "            del mask\n",
    "            \n",
    "        logging.info(\">>>> Now we analyze its memory usage <<<<\")\n",
    "        vfe_df = pd.read_html(prof.display()._repr_html_())[0]\n",
    "        vfe_df.columns = [i[0] for i in vfe_df.columns]\n",
    "        vfe_df[['active_bytes', 'reserved_bytes']] = rearrange_df(vfe_df, profile_dict['vfe_df'], batch_size)\n",
    "        logging.info('\\n' + str(vfe_df))\n",
    "        \n",
    "    else:\n",
    "        out = one_data\n",
    "        logging.info(\"This model does not have a vfe\")\n",
    "        \n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"|||||||||||||||||||||||||||||||||||| 3D backbone Part ||||||||||||||||||||||||||||||||||||\")\n",
    "    logging.info(\"\\n\")\n",
    "    if backbone_3d is not None:\n",
    "        logging.info(\"----------------------- Analyzing on {} -----------------------\".format(model.backbone_3d.__class__.__name__))\n",
    "        with LineProfiler(backbone_3d.forward) as prof2:\n",
    "            start_time = time.time()\n",
    "            out2 = backbone_3d.forward(out)\n",
    "            total_time = (time.time() - start_time)\n",
    "            logging.info(\">>>>>>>>>>>>>> The run time for {} is {} ms\".format(model.backbone_3d.__class__.__name__, profile_dict['backbone_3d_time'] * 1000))\n",
    "        \n",
    "        if \"Voxel\" in backbone_3d.__class__.__name__:\n",
    "        # Analyze specific process\n",
    "            voxel_features, voxel_coords = out['voxel_features'], out['voxel_coords']\n",
    "            batch_size = out['batch_size']\n",
    "            input_sp_tensor = spconv.SparseConvTensor(\n",
    "                    features=voxel_features,\n",
    "                    indices=voxel_coords.int(),\n",
    "                    spatial_shape=model.backbone_3d.sparse_shape,\n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "            x = model.backbone_3d.conv_input(input_sp_tensor)\n",
    "            start_time = time.time()\n",
    "            conv1 = backbone_3d.conv1(x)\n",
    "            total_time = (time.time() - start_time)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for conv1 in {} is {} ms\".format(model.backbone_3d.__class__.__name__, profile_dict['spconv1_t'] * 1000))\n",
    "            start_time2 = time.time()\n",
    "            conv2 = backbone_3d.conv2(conv1)\n",
    "            total_time2 = (time.time() - start_time2)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for conv2 in {} is {} ms\".format(model.backbone_3d.__class__.__name__, profile_dict['spconv2_t'] * 1000))\n",
    "            start_time3 = time.time()\n",
    "            conv3 = backbone_3d.conv3(conv2)\n",
    "            total_time3 = (time.time() - start_time3)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for conv3 in {} is {} ms\".format(model.backbone_3d.__class__.__name__, profile_dict['spconv3_t'] * 1000))\n",
    "            start_time4 = time.time()\n",
    "            conv4 = backbone_3d.conv4(conv3)\n",
    "            end_time = time.time()\n",
    "            total_time4 = (end_time - start_time4)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for conv4 in {} is {} ms\".format(model.backbone_3d.__class__.__name__, profile_dict['spconv4_t'] * 1000))\n",
    "            logging.info(\">>>>>>>>>>>>>> The average running time of the combined convolutinal blocks is {} ms \".format((end_time - start_time) * 1000 / batch_size))\n",
    "            \n",
    "            del voxel_features, voxel_coords\n",
    "            del input_sp_tensor\n",
    "            del x\n",
    "            del conv1\n",
    "            del conv2\n",
    "            del conv3\n",
    "            del conv4\n",
    "        \n",
    "        elif \"Point\" in backbone_3d.__class__.__name__:\n",
    "            \n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime of SA modules in {} is {} ms\".format(backbone_3d.__class__.__name__, profile_dict['MSG_SA_time'] * 1000))\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime of FP modules in {} is {} ms\".format(backbone_3d.__class__.__name__, profile_dict['MSG_FP_time'] * 1000))\n",
    "            \n",
    "        logging.info(\"\\n\")\n",
    "        logging.info(\">>>> Now we analyze its memory usage <<<<\")\n",
    "        \n",
    "        backbone_3d_df = pd.read_html(prof2.display()._repr_html_())[0]\n",
    "        backbone_3d_df.columns = [i[0] for i in backbone_3d_df.columns]\n",
    "        backbone_3d_df[['active_bytes', 'reserved_bytes']] = rearrange_df(backbone_3d_df, profile_dict['backbone_3d_df'], batch_size)\n",
    "        logging.info('\\n' + str(backbone_3d_df))\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        out2 = out\n",
    "        logging.info(\"This model does not have a 3d backbone\")\n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"|||||||||||||||||||||||||||||||||||| Map2Bev Part ||||||||||||||||||||||||||||||||||||\")\n",
    "    logging.info(\"\\n\")\n",
    "    if map2bev is not None:\n",
    "        logging.info(\"----------------------- Analyzing on {} -----------------------\".format(model.map_to_bev_module.__class__.__name__))\n",
    "        with LineProfiler(map2bev.forward) as prof3:\n",
    "            start_time = time.time()\n",
    "            out3 = map2bev.forward(out2)\n",
    "            total_time = (time.time() - start_time)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for {} is {} ms\".format(model.map_to_bev_module.__class__.__name__, profile_dict['map2bev_time'] * 1000))\n",
    "        logging.info(\"\\n\")\n",
    "        logging.info(\">>>> Now we analyze its memory usage <<<<\")    \n",
    "        map2bev_df = pd.read_html(prof3.display()._repr_html_())[0]\n",
    "        map2bev_df.columns = [i[0] for i in map2bev_df.columns]\n",
    "        map2bev_df[['active_bytes', 'reserved_bytes']] = rearrange_df(map2bev_df, profile_dict['map2bev_df'], batch_size)\n",
    "        logging.info('\\n' + str(map2bev_df))\n",
    "    else:\n",
    "        out3 = out2\n",
    "        logging.info(\"This model does not rely on BEV\")\n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"|||||||||||||||||||||||||||||||||||| PFE Part ||||||||||||||||||||||||||||||||||||\")\n",
    "    logging.info(\"\\n\")\n",
    "    if pfe is not None:\n",
    "        logging.info(\"----------------------- Analyzing on {} -----------------------\".format(pfe.__class__.__name__))\n",
    "        with LineProfiler(pfe.forward) as prof4:\n",
    "            start_time = time.time()\n",
    "            out4 = pfe.forward(out3)\n",
    "            total_time = (time.time() - start_time)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for {} is {} ms\".format(pfe.__class__.__name__, profile_dict['pfe_time'] * 1000))\n",
    "        logging.info(\"*************************** The overall memory analyze of {} ***************************\".format(pfe.__class__.__name__))\n",
    "        pfe_df = pd.read_html(prof4.display()._repr_html_())[0]\n",
    "        pfe_df.columns = [i[0] for i in pfe_df.columns]\n",
    "        pfe_df[['active_bytes', 'reserved_bytes']] = rearrange_df(pfe_df, profile_dict['pfe_df'], batch_size)\n",
    "        logging.info('\\n' + str(pfe_df))\n",
    "        \n",
    "        with LineProfiler(pfe.get_sampled_points) as grouping:\n",
    "            start_time_grouping = time.time()\n",
    "            keypoints = pfe.get_sampled_points(out3)\n",
    "            total_time_grouping = time.time() - start_time_grouping\n",
    "            logging.info(\">>>>>>>>>>>>>> The total time for the grouping process in {} is {} ms.\".format(pfe.__class__.__name__, profile_dict['pfe_grouping_t'] * 1000))\n",
    "        logging.info(\"\\n\")\n",
    "        logging.info(\"************** Analyzing memory usage of grouping process **************\")\n",
    "        pfe_grouping_df = pd.read_html(grouping.display()._repr_html_())[0]\n",
    "        pfe_grouping_df.columns = [i[0] for i in pfe_grouping_df.columns]\n",
    "        pfe_grouping_df[['active_bytes', 'reserved_bytes']] = rearrange_df(pfe_grouping_df, profile_dict['pfe_grouping_df'], batch_size)\n",
    "        logging.info(\"\\n\" + str(pfe_grouping_df))\n",
    "        point_features_list = []\n",
    "        if 'bev' in pfe.model_cfg.FEATURES_SOURCE:\n",
    "            with LineProfiler(pfe.interpolate_from_bev_features) as bev:\n",
    "                start_time_bev = time.time()\n",
    "                point_bev_features = pfe.interpolate_from_bev_features(\n",
    "                    keypoints, out3['spatial_features'], out3['batch_size'],\n",
    "                    bev_stride=out3['spatial_features_stride']\n",
    "                )\n",
    "                total_time_bev = time.time() - start_time_bev\n",
    "                logging.info(\">>>>>>>>>>>>>> The total time of getting bev features is {} ms\".format(profile_dict['pfe_bev_feature_t'] * 1000))\n",
    "            logging.info(\"\\n\")\n",
    "            logging.info(\"************** Analyzing memory usage of getting bev features **************\")\n",
    "            pfe_bev_df = pd.read_html(bev.display()._repr_html_())[0]\n",
    "            pfe_bev_df.columns = [i[0] for i in pfe_bev_df.columns]\n",
    "            pfe_bev_df[['active_bytes', 'reserved_bytes']] = rearrange_df(pfe_bev_df, profile_dict['pfe_bev_df'], batch_size)\n",
    "            logging.info('\\n' + str(pfe_bev_df))\n",
    "            point_features_list.append(point_bev_features)\n",
    "            \n",
    "            del point_bev_features\n",
    "            \n",
    "        batch_size = out3['batch_size']\n",
    "        new_xyz = keypoints[:, 1:4].contiguous()\n",
    "        new_xyz_batch_cnt = new_xyz.new_zeros(batch_size).int()\n",
    "        for k in range(batch_size):\n",
    "            new_xyz_batch_cnt[k] = (keypoints[:, 0] == k).sum()\n",
    "        if 'raw_points' in pfe.model_cfg.FEATURES_SOURCE:\n",
    "            raw_points = out3['points']\n",
    "            with LineProfiler(pfe.aggregate_keypoint_features_from_one_source) as raw:\n",
    "                start_time_raw = time.time()\n",
    "                pooled_features = pfe.aggregate_keypoint_features_from_one_source(\n",
    "                    batch_size=batch_size, aggregate_func=pfe.SA_rawpoints,\n",
    "                    xyz=raw_points[:, 1:4],\n",
    "                    xyz_features=raw_points[:, 4:].contiguous() if raw_points.shape[1] > 4 else None,\n",
    "                    xyz_bs_idxs=raw_points[:, 0],\n",
    "                    new_xyz=new_xyz, new_xyz_batch_cnt=new_xyz_batch_cnt,\n",
    "                    filter_neighbors_with_roi=pfe.model_cfg.SA_LAYER['raw_points'].get('FILTER_NEIGHBOR_WITH_ROI', False),\n",
    "                    radius_of_neighbor=pfe.model_cfg.SA_LAYER['raw_points'].get('RADIUS_OF_NEIGHBOR_WITH_ROI', None),\n",
    "                    rois=out3.get('rois', None)\n",
    "                )\n",
    "                total_time_raw = time.time() - start_time_raw\n",
    "                logging.info(\">>>>>>>>>>>>>> The total runtime for pooling features from raw points is {} ms\".format(profile_dict['pfe_raw_feature_t'] * 1000))\n",
    "            logging.info(\"\\n\")\n",
    "            logging.info(\"************** Analyzing memory usage of getting raw_points features **************\")\n",
    "            pfe_raw_df = pd.read_html(raw.display()._repr_html_())[0]\n",
    "            pfe_raw_df.columns = [i[0] for i in pfe_raw_df.columns]\n",
    "            pfe_raw_df[['active_bytes', 'reserved_bytes']] = rearrange_df(pfe_raw_df, profile_dict['pfe_raw_df'], batch_size)\n",
    "            logging.info('\\n' + str(pfe_raw_df))\n",
    "            point_features_list.append(pooled_features)\n",
    "            del pooled_features\n",
    "            \n",
    "        for k, src_name in enumerate(pfe.SA_layer_names):\n",
    "            cur_coords = out3['multi_scale_3d_features'][src_name].indices\n",
    "            cur_features = out3['multi_scale_3d_features'][src_name].features.contiguous()\n",
    "            xyz = common_utils.get_voxel_centers(\n",
    "                cur_coords[:, 1:4], downsample_times=pfe.downsample_times_map[src_name],\n",
    "                voxel_size=pfe.voxel_size, point_cloud_range=pfe.point_cloud_range\n",
    "            )\n",
    "            with LineProfiler(pfe.aggregate_keypoint_features_from_one_source) as mlp:\n",
    "                start_time_mlp = time.time()\n",
    "                pooled_features = pfe.aggregate_keypoint_features_from_one_source(\n",
    "                    batch_size=batch_size, aggregate_func=pfe.SA_layers[k],\n",
    "                    xyz=xyz.contiguous(), xyz_features=cur_features, xyz_bs_idxs=cur_coords[:, 0],\n",
    "                    new_xyz=new_xyz, new_xyz_batch_cnt=new_xyz_batch_cnt,\n",
    "                    filter_neighbors_with_roi=pfe.model_cfg.SA_LAYER[src_name].get('FILTER_NEIGHBOR_WITH_ROI', False),\n",
    "                    radius_of_neighbor=pfe.model_cfg.SA_LAYER[src_name].get('RADIUS_OF_NEIGHBOR_WITH_ROI', None),\n",
    "                    rois=out3.get('rois', None)\n",
    "                )\n",
    "                total_time_mlp = time.time() - start_time_mlp\n",
    "                logging.info(\">>>>>>>>>>>>>> The total time of mlp {} is {} ms\".format(src_name, profile_dict['pfe_mlp_t'][k]))\n",
    "            logging.info(\"\\n\")\n",
    "            logging.info(\"************** Analyzing memory usage of getting features from the mlp {} **************\".format(src_name))\n",
    "            pfe_mlp_df = pd.read_html(mlp.display()._repr_html_())[0]\n",
    "            pfe_mlp_df.columns = [i[0] for i in pfe_mlp_df.columns]\n",
    "            pfe_mlp_df[['active_bytes', 'reserved_bytes']] = rearrange_df(pfe_mlp_df, profile_dict['pfe_mlp_df'][k], batch_size)\n",
    "            logging.info('\\n' + str(pfe_mlp_df))\n",
    "            point_features_list.append(pooled_features)\n",
    "            \n",
    "        del point_features_list\n",
    "        del keypoints\n",
    "        del new_xyz\n",
    "        del new_xyz_batch_cnt\n",
    "        del cur_coords\n",
    "        del cur_features\n",
    "        del xyz\n",
    "        del pooled_features\n",
    "    else:\n",
    "        out4 = out3\n",
    "        logging.info(\"This model does not have a pfe\")\n",
    "    \n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"|||||||||||||||||||||||||||||||||||| 2D backbone Part ||||||||||||||||||||||||||||||||||||\")\n",
    "    logging.info(\"\\n\")\n",
    "    if backbone_2d is not None:\n",
    "        logging.info(\"----------------------- Analyzing on {} -----------------------\".format(backbone_2d.__class__.__name__))\n",
    "        with LineProfiler(backbone_2d.forward) as prof5:\n",
    "            start_time = time.time()\n",
    "            out5 = backbone_2d.forward(out4)\n",
    "            total_time = (time.time() - start_time)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for {} is {} ms\".format(backbone_2d.__class__.__name__, profile_dict['backbone_2d_time'] * 1000))\n",
    "        logging.info(\"\\n\")\n",
    "        logging.info(\">>>> Now we analyze its memory usage <<<<\")\n",
    "        backbone_2d_df = pd.read_html(prof5.display()._repr_html_())[0]\n",
    "        backbone_2d_df.columns = [i[0] for i in backbone_2d_df.columns]\n",
    "        backbone_2d_df[['active_bytes', 'reserved_bytes']] = rearrange_df(backbone_2d_df, profile_dict['backbone_2d_df'], batch_size)\n",
    "        logging.info('\\n' + str(backbone_2d_df))\n",
    "    else:\n",
    "        out5 = out4\n",
    "        logging.info(\"This model does not have a 2D backbone\")\n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"|||||||||||||||||||||||||||||||||||| Dense Head Part ||||||||||||||||||||||||||||||||||||\")\n",
    "    logging.info(\"\\n\")\n",
    "    if dense_head is not None:\n",
    "        logging.info(\"----------------------- Analyzing on {} -----------------------\".format(dense_head.__class__.__name__))\n",
    "        with LineProfiler(dense_head.forward) as prof6:\n",
    "            start_time = time.time()\n",
    "            out6 = dense_head.forward(out5)\n",
    "            total_time = (time.time() - start_time)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for {} is {} ms\".format(dense_head.__class__.__name__, profile_dict['dense_head_time'] * 1000))\n",
    "        if dense_head.__class__.__name__ == \"CenterHead\":\n",
    "            spatial_features_2d = out5['spatial_features_2d']\n",
    "            x = dense_head.shared_conv(spatial_features_2d)\n",
    "            pred_dicts = []\n",
    "            start_time = time.time()\n",
    "            for head in dense_head.heads_list:\n",
    "                pred_dicts.append(head(x))\n",
    "            total_time = time.time() - start_time\n",
    "            logging.info(\">>>>>>>>>>>>>> The total runtime of separateHeads is {} ms\".format(profile_dict['center_head_time'] * 1000))\n",
    "            logging.info(\">>>>>>>>>>>>>> Number of separated_heads in the heads_list: {}\".format(len(dense_head.heads_list)))\n",
    "            logging.info(\">>>>>>>>>>>>>> The average runtime of each separateHead is {} ms\".format(profile_dict['center_head_time'] * 1000 / len(dense_head.heads_list)))\n",
    "            del spatial_features_2d\n",
    "            del x \n",
    "            del pred_dicts\n",
    "        logging.info(\"\\n\")\n",
    "        logging.info(\">>>> Now we analyze its memory usage <<<<\")\n",
    "        dense_head_df = pd.read_html(prof6.display()._repr_html_())[0]\n",
    "        dense_head_df.columns = [i[0] for i in dense_head_df.columns]\n",
    "        dense_head_df[['active_bytes', 'reserved_bytes']] = rearrange_df(dense_head_df, profile_dict['dense_head_df'], batch_size)\n",
    "        logging.info('\\n' + str(dense_head_df))\n",
    "    else:\n",
    "        out6 = out5\n",
    "        logging.info(\"This model does not have a dense head.\")  \n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"|||||||||||||||||||||||||||||||||||| Point Head Part ||||||||||||||||||||||||||||||||||||\")\n",
    "    logging.info(\"\\n\")\n",
    "    if point_head is not None:\n",
    "        logging.info(\"----------------------- Analyzing on {} -----------------------\".format(point_head.__class__.__name__))\n",
    "        with LineProfiler(point_head.forward) as ph:\n",
    "            start_time = time.time()\n",
    "            out7 = point_head.forward(out6)\n",
    "            total_time = (time.time() - start_time)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for {} is {} ms\".format(point_head.__class__.__name__, profile_dict['point_head_time'] * 1000))\n",
    "        logging.info(\"************** Analyzing memory usage of {}**************\".format(point_head.__class__.__name__))\n",
    "        point_head_df = pd.read_html(ph.display()._repr_html_())[0]\n",
    "        point_head_df.columns = [i[0] for i in point_head_df.columns]\n",
    "        point_head_df[['active_bytes', 'reserved_bytes']] = rearrange_df(point_head_df, profile_dict['point_head_df'], batch_size)\n",
    "        logging.info('\\n' + str(point_head_df))\n",
    "    else:\n",
    "        out7 = out6\n",
    "        logging.info(\"This model does not have a point head\")\n",
    "\n",
    "    \n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"|||||||||||||||||||||||||||||||||||| ROI Head Part ||||||||||||||||||||||||||||||||||||\")\n",
    "    logging.info(\"\\n\")\n",
    "    if roi_head is not None:\n",
    "        logging.info(\"----------------------- Analyzing on {} -----------------------\".format(roi_head.__class__.__name__))\n",
    "        with LineProfiler(roi_head.forward) as roi:\n",
    "            start_time = time.time()\n",
    "            print(out7.keys())\n",
    "            out8 = roi_head.forward(out7)\n",
    "            total_time = (time.time() - start_time)\n",
    "            logging.info(\">>>>>>>>>>>>>> The runtime for {} is {} ms\".format(roi_head.__class__.__name__, profile_dict['roi_head_time'] * 1000))\n",
    "        logging.info(\"\\n\")\n",
    "\n",
    "        logging.info(\"************** Analyzing memory usage of {}**************\".format(roi_head.__class__.__name__))\n",
    "        roi_head_df = pd.read_html(roi.display()._repr_html_())[0]\n",
    "        roi_head_df.columns = [i[0] for i in roi_head_df.columns]\n",
    "        roi_head_df[['active_bytes', 'reserved_bytes']] = rearrange_df(roi_head_df, profile_dict['roi_head_df'], batch_size)\n",
    "        logging.info('\\n' + str(roi_head_df))\n",
    "        \n",
    "        if 'PV' in roi_head.__class__.__name__:\n",
    "            with LineProfiler(roi_head.roi_grid_pool) as pooling:\n",
    "                start_time_pooling = time.time()\n",
    "                pooled_features = roi_head.roi_grid_pool(out7)\n",
    "                total_time_pooling = time.time() - start_time_pooling\n",
    "                logging.info(\">>>>>>>>>>>>>> The runtime for ROI grid pooling in {} is {} ms\".format(roi_head.__class__.__name__, profile_dict['roi_pooling_time']))\n",
    "            logging.info(\"\\n\")\n",
    "            logging.info(\"************** Analyzing memory usage of ROI grid pooling **************\")\n",
    "            roi_pooling_df = pd.read_html(pooling.display()._repr_html_())[0]\n",
    "            roi_pooling_df.columns = [i[0] for i in roi_pooling_df.columns]\n",
    "            roi_pooling_df[['active_bytes', 'reserved_bytes']] = rearrange_df(roi_pooling_df, profile_dict['roi_pooling_df'], batch_size)\n",
    "            logging.info('\\n' + str(roi_pooling_df))\n",
    "        \n",
    "        elif 'PointRCNN' in roi_head.__class__.__name__:\n",
    "            play_data = out7.copy()\n",
    "            with LineProfiler(roi_head.proposal_layer) as proposal:\n",
    "                start_time = time.time()\n",
    "                targets_dict = roi_head.proposal_layer(play_data, nms_config=roi_head.model_cfg.NMS_CONFIG['TRAIN' if roi_head.training else 'TEST'])\n",
    "                end = time.time() - start_time\n",
    "                logging.info(\">>>>>>>>>>>>>> The runtime for The Proposal Layer in {} is {} ms\".format(roi_head.__class__.__name__, profile_dict['point_rcnn_proposal_time']))\n",
    "            logging.info(\"\\n\")\n",
    "            logging.info(\"************** Analyzing memory usage of PointRCNN Proposal Layer **************\")\n",
    "            point_rcnn_proposal_df = pd.read_html(proposal.display()._repr_html_())[0]\n",
    "            point_rcnn_proposal_df.columns = [i[0] for i in point_rcnn_proposal_df.columns]\n",
    "            point_rcnn_proposal_df[['active_bytes', 'reserved_bytes']] = rearrange_df(point_rcnn_proposal_df, profile_dict['point_rcnn_proposal_df'], batch_size)\n",
    "            logging.info('\\n' + str(point_rcnn_proposal_df))\n",
    "            del play_data\n",
    "    else:\n",
    "        out8 = out7\n",
    "        logging.info(\"This model does not have a ROI head\")\n",
    "    \n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"------------------------------------------------------ Finish profiling ------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling required models\n",
    "**Remember to ```restart``` the jupyter notebook before profiling a new model, because jupyter notebook will store previous loaded data (ie. if you run the same cell multiple times, the memory usage will be accumulated), which might cause a large memory usage.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiling SECOND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:05:12,490   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:05:13,985   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:13,985   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:05:15,681   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:05:15,681   INFO  Loading Waymo dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'metadata', 'batch_size'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:05:17,294   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:17,294   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:17,295   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:05:17,295   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:05:22,044   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:05:22,044   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:05:22,044   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:05:23,452   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:23,452   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:23,452   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:23,453   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:05:23,453   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:05:23,453   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:05:23,923   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:05:23,923   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:05:23,923   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:05:23,923   INFO  Loading Waymo dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'metadata', 'batch_size'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:05:25,759   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:25,759   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:25,759   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:25,759   INFO  Total skipped info 0\n",
      "2023-01-07 04:05:25,761   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:05:25,761   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:05:25,761   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:05:25,761   INFO  Total samples for Waymo dataset: 39987\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(filename=\"./profiling_results/second_profiling.log\", level=logging.INFO)\n",
    "profile_dict = profiling_one_batch(\"second\")\n",
    "profiling_multi_batch(\"second\", 2, profile_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiling PointPillar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 03:51:21,572   INFO  Loading Waymo dataset\n",
      "2023-01-07 03:51:21,572   INFO  Loading Waymo dataset\n",
      "2023-01-07 03:51:22,958   INFO  Total skipped info 0\n",
      "2023-01-07 03:51:22,958   INFO  Total skipped info 0\n",
      "2023-01-07 03:51:22,959   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 03:51:22,959   INFO  Total samples for Waymo dataset: 39987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'metadata', 'batch_size'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 03:51:24,405   INFO  Loading Waymo dataset\n",
      "2023-01-07 03:51:24,405   INFO  Loading Waymo dataset\n",
      "2023-01-07 03:51:24,405   INFO  Loading Waymo dataset\n",
      "2023-01-07 03:51:25,721   INFO  Total skipped info 0\n",
      "2023-01-07 03:51:25,721   INFO  Total skipped info 0\n",
      "2023-01-07 03:51:25,721   INFO  Total skipped info 0\n",
      "2023-01-07 03:51:25,722   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 03:51:25,722   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 03:51:25,722   INFO  Total samples for Waymo dataset: 39987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'metadata', 'batch_size'])\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(filename=\"./profiling_results/pointpillar_profiling.log\", level=logging.INFO)\n",
    "profile_dict = profiling_one_batch(\"pointpillar_1x\")\n",
    "profiling_multi_batch(\"pointpillar_1x\", 2, profile_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiling Centerpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:01:32,359   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:01:33,854   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:33,855   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:01:36,361   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:01:36,361   INFO  Loading Waymo dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'metadata', 'batch_size'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:01:37,978   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:37,978   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:37,979   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:01:37,979   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:01:43,370   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:01:43,370   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:01:43,370   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:01:44,819   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:44,819   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:44,819   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:44,821   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:01:44,821   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:01:44,821   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:01:45,296   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:01:45,296   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:01:45,296   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:01:45,296   INFO  Loading Waymo dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'metadata', 'batch_size'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:01:47,140   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:47,140   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:47,140   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:47,140   INFO  Total skipped info 0\n",
      "2023-01-07 04:01:47,141   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:01:47,141   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:01:47,141   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:01:47,141   INFO  Total samples for Waymo dataset: 39987\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(filename=\"./profiling_results/centerpoint_profiling.log\", level=logging.INFO)\n",
    "profile_dict = profiling_one_batch(\"centerpoint\")\n",
    "profiling_multi_batch(\"centerpoint\", 2, profile_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiling PV_RCNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:03:40,516   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:03:42,003   INFO  Total skipped info 0\n",
      "2023-01-07 04:03:42,004   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:03:43,658   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:03:43,658   INFO  Loading Waymo dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'metadata', 'batch_size'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:03:45,280   INFO  Total skipped info 0\n",
      "2023-01-07 04:03:45,280   INFO  Total skipped info 0\n",
      "2023-01-07 04:03:45,281   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:03:45,281   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:03:56,449   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:03:56,449   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:03:56,449   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:03:57,937   INFO  Total skipped info 0\n",
      "2023-01-07 04:03:57,937   INFO  Total skipped info 0\n",
      "2023-01-07 04:03:57,937   INFO  Total skipped info 0\n",
      "2023-01-07 04:03:57,939   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:03:57,939   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:03:57,939   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:03:58,507   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:03:58,507   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:03:58,507   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:03:58,507   INFO  Loading Waymo dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'metadata', 'batch_size'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:04:00,439   INFO  Total skipped info 0\n",
      "2023-01-07 04:04:00,439   INFO  Total skipped info 0\n",
      "2023-01-07 04:04:00,439   INFO  Total skipped info 0\n",
      "2023-01-07 04:04:00,439   INFO  Total skipped info 0\n",
      "2023-01-07 04:04:00,441   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:04:00,441   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:04:00,441   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:04:00,441   INFO  Total samples for Waymo dataset: 39987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'metadata', 'batch_size', 'voxel_features', 'encoded_spconv_tensor', 'encoded_spconv_tensor_stride', 'multi_scale_3d_features', 'multi_scale_3d_strides', 'spatial_features', 'spatial_features_stride', 'point_features_before_fusion', 'point_features', 'point_coords', 'spatial_features_2d', 'batch_cls_preds', 'batch_box_preds', 'cls_preds_normalized', 'point_cls_scores'])\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(filename=\"./profiling_results/pv_rcnn_profiling.log\", level=logging.INFO)\n",
    "profile_dict = profiling_one_batch(\"pv_rcnn\")\n",
    "# profiling_multi_batch(\"pv_rcnn\", 4, profile_dict)  -- CUDA out of memory!!!!!\n",
    "profiling_multi_batch(\"pv_rcnn\", 2, profile_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiling Point_RCNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:06:12,718   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:06:14,198   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:14,199   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:06:16,414   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:06:16,414   INFO  Loading Waymo dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'metadata', 'batch_size'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:06:18,020   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:18,020   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:18,021   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:06:18,021   INFO  Total samples for Waymo dataset: 39987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:06:41,847   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:06:41,847   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:06:41,847   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:06:43,189   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:43,189   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:43,189   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:43,191   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:06:43,191   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:06:43,191   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:06:43,375   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:06:43,375   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:06:43,375   INFO  Loading Waymo dataset\n",
      "2023-01-07 04:06:43,375   INFO  Loading Waymo dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'metadata', 'batch_size'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 04:06:45,207   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:45,207   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:45,207   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:45,207   INFO  Total skipped info 0\n",
      "2023-01-07 04:06:45,209   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:06:45,209   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:06:45,209   INFO  Total samples for Waymo dataset: 39987\n",
      "2023-01-07 04:06:45,209   INFO  Total samples for Waymo dataset: 39987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "PointRCNN is so annoying.\n",
      "dict_keys(['sample_idx', 'points', 'frame_id', 'gt_boxes', 'use_lead_xyz', 'metadata', 'batch_size', 'point_features', 'point_coords', 'point_cls_scores', 'batch_cls_preds', 'batch_box_preds', 'batch_index', 'cls_preds_normalized'])\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(filename=\"./profiling_results/point_rcnn_profiling.log\", level=logging.INFO)\n",
    "profile_dict = profiling_one_batch(\"point_rcnn\")\n",
    "profiling_multi_batch(\"point_rcnn\", 2, profile_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. More experiments for specific operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a272236ba80e9867e5d49a1e2073855594dfbe1e8ace94f4d42516ee70cf12a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
